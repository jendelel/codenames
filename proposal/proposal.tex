\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}

% custom commands
\usepackage{color}
\newcommand{\comment}[2]{{#2}}
\newcommand{\TODO}[1]{\{\textbf{TODO: }\textcolor{red}{#1}\}}
% /custom commands

\title{Learning to communicate concisely\\{\large \textit{Deep Learning Course Project Proposal}}}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Luk\'{a}\v{s} Jendele\thanks{Equal contributions.}\\
  ETH Z\"{u}rich\\
  \href{mailto:jendelel@ethz.ch}{\texttt{jendelel@ethz.ch}}\\
  \And
  Ondrej Skopek\textsuperscript{$\ast$}\\
  ETH Z\"{u}rich\\
  \href{mailto:oskopek@ethz.ch}{\texttt{oskopek@ethz.ch}}\\
  \And
  Vignesh Ram Somnath\textsuperscript{$\ast$}\\
  ETH Z\"{u}rich\\
  \href{mailto:vsomnath@ethz.ch}{\texttt{vsomnath@ethz.ch}}
}

\begin{document}
\maketitle

%%%%%%
Deep Reinforcement Learning has been the driving force behind several achievements in game-playing agents over the past few years. Notable examples are AlphaGo \cite{alphago}, AlphaGo Zero \cite{alphagozero}, AlphaZero \cite{alphazero}, Capture The Flag \cite{ctf}, and many others. In this project, we aim to extend the idea behind AlphaZero to train AI agents to play the game of Codenames\footnote{\url{https://boardgamegeek.com/boardgame/178900/codenames}}.

Codenames is a multiplayer card game originally designed for four agents: two teams (red, blue) with two agents --- a ``spymaster'' and ``guesser''. The game board is a $5 \times 5$ grid of nouns, each word belonging to one of four groups: red team, blue team, neutral, and an assassin. The teams alternate with taking turns: a spymaster gives a ``clue'' (single word that is not contained in the words on the board and a number designating the number of words associated with the clue). The team of the spymaster then goes on to iteratively guess words on the board. If they guessed a card of their team, they get another turn (with a maximum number of turns given by the clue word number + 1). If they guess a neutral card or a card of the other team, their turn ends. Guessing an assassin card makes the team lose immediately. This constitutes one round of the game.

We model the game in the following. We assume a fixed vocabulary $V_B$ for the words on the board\footnote{From: \url{https://github.com/jbowens/codenames/}}, and a different (larger, more variant) fixed vocabulary $V_C$ for giving/receiving clues by the agents\footnote{\url{https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md}} \cite{fasttext}. We assume $V_B \subseteq V_C$. For practical purposes, we only train the two agents for the red team, and let them play one round per training step, because Codenames is easier to evaluate than e.g.~chess on a turn by turn basis (guessing a word of your team gives positive rewards, otherwise you get a negative reward).

Our proof of concept implementation in PyTorch will be made available, and the API will enable the creation of bots to play a game of humans vs.~bots, bots vs.~bots, or humans with bots, with pretrained or custom weights. We expect the game to train faster than AlphaZero, since our rounds are shorter and the rewards are more expressive.

To evaluate our trained agents, we propose the following experiments. First, we compare our agents to a baseline that optimizes a custom loss function in a pretrained word embedding space to find clues, and looks for nearest neighbors in the same space for guessing. Secondly, we compare our spymaster to the implementation of a spymaster by \cite{codenamesai}. Optionally, we play a round of games using a human control group: both human vs.~bot games and mixed human-bot games.

To summarize, our contributions will be the following:
\begin{enumerate}
    \item Design and implement deep RL agents in PyTorch that learn to play Codenames by self-play.
    \item Implement a simple game interface to play Codenames against/with trained agents.
    \item Evaluate the performance of our agents in several diverse experiments, optionally followed by a short ablation study.
\end{enumerate}


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
